# gradcam-yolo
Abstract— This paper implements and investigates popular
adversarial attacks on the YOLOv5 Object Detection algorithm.
The paper explores the vulnerability of the YOLOv5 to
adversarial attacks in the context of traffic and road sign
detection. The paper investigates the impact of different types of
attacks, including the Limited-memory Broyden-FletcherGoldfarb-Shanno (L-BFGS), the Fast Gradient Sign Method
(FGSM) attack, the Carlini and Wagner (C&W) attack, the
Basic Iterative Method (BIM) attack, the Projected Gradient
Descent (PGD) attack, One Pixel Attack, and the Universal
Adversarial Perturbations attack on the accuracy of YOLOv5
in detecting traffic and road signs. The results show that
YOLOv5 is susceptible to these attacks, with misclassification
rates increasing as the magnitude of the perturbations increases.
We also explain the results using saliency maps. The findings of
this paper have important implications for the safety and
reliability of object detection algorithms used in traffic and
transportation systems, highlighting the need for more robust
and secure models to ensure their effectiveness in real-world
applications.
Keywords— Adversarial Attacks, Adversarial Training,
Traffic Sign Detection, YOLOv5
![image](https://user-images.githubusercontent.com/13884479/235002445-696edd66-de9d-4648-919a-be51a14a1d0c.png)


Analysing how adversarial attacks aaffect YOLO Object Detection and visualising GradCAM

![image](https://user-images.githubusercontent.com/13884479/235001938-f4ae31e4-9ee3-41e8-9de5-4dff1ab62091.png)

![image](https://user-images.githubusercontent.com/13884479/235002034-9c3d1d45-a6cc-4157-b823-728104a18570.png)
